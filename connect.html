<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
      The Data Guru
    </title>
    <link rel="stylesheet" type="text/css" href="css/main.css">
    <link href='//fonts.googleapis.com/css?family=Montserrat:thin,extra-light,light,100,200,300,400,500,600,700,800' 
rel='stylesheet' type='text/css'>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script>
      $(document).ready(function () {
        $('body').fadeIn(1000);
      });
    </script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VSJE2TEEHZ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-VSJE2TEEHZ');
    </script>
  </head>

  <body style="display:none;">
    <h1 class="align-centre">The Data Guru. Connect.</h1>
    <p>
      <a href="https://www.haskellers.com/user/5402"><img src="https://www.haskellers.com/static/badge.png" alt="I'm a Haskeller">
      </a>
    </p>
    <div data-iframe-width="150" data-iframe-height="270" data-share-badge-id="033243a8-ea7a-4b04-b69f-d37b554797e3" data-share-badge-host="https://www.credly.com"></div><script type="text/javascript" async src="//cdn.credly.com/assets/utilities/embed.js"></script>
  <p><a href=https://www.linkedin.com/in/roblongleeds/>
    <img src="images/LinkedIn_Logo_40px.png" alt="Connnect on LinkedIn"> </a>
  </p><p>
    <a href=https://stats.stackexchange.com/users/7486/robert-long/>
    <img src="images/so-logo_80px.png" alt="Connnect on Stack Exchange"> </a>
</p><p>
    <a href="mailto:rob@thedataguru.net">
    <img src="images/Emailicon01.png" alt="Email me"> </a>  
</p><p>
    <aa>or Call or WhatsApp me on <a href="tel:+447400986194">07400 986 194</a> </aa>
</p>
<p style="text-align: center; margin-top: 2rem;">
  <a href="stat-reviews.html" style="color: #FFD700;">← View Statistical Reviews of Cardiology Journal Papers</a>
</p>

<h1>Connections:</h1>
      <h2> Big Data</h2>
      <p> Data has become critical to human civilisation in recent years: It has changed how we are educated, how we are entertained, and more generally how we live. It informs how we experience others, how we conduct business, and the wider world around us. It is the lifeblood of our rapidly growing digital, and increasingly physical, existence.</p>
      <p>Big Data is, simply put, a set of storage and processing methodologies. It is a term used to describe the rapid growth and availability of data, whose size (Volume), complexity (Variability), and rate of growth (Velocity) make them difficult or even impossible to be managed and analyzed using conventional software tools and technologies.</p>
      <h3>Volume:</h3>
      <p>Whereas in traditional statistical / data analysis, a dataset with a few hundred thousand records and several hundred variables would be considered “big” and would present some challenges for timely analysis, these days, datasets of billions of records and thousands of variables is not uncommon. Storage, retrieval and analysis using conventional tools in such a case is impossible, and a distributed model must be adopted. Big Data tools and technologies such as Hadoop and Spark are designed from the ground up for these purposes.</p>
      <h3>Variability:</h3>
      <p>Whereas in traditional statistical / data analysis, datasets are usually well-defined and structured in a tabular format, often with relationships between data that are well described/modelled/analysed with SQL, in the era of Big Data we also handle unstructured data such as image files and video files.  Storage of such data cannot easily be achieved using SQL, and therefore the adoption of “NoSQL”  (“Not Only SQL”) databases such as MongoDB and Cassandra is becoming very common.</p>
      <h3>Velocity:</h3>
      <p>Many people may be familiar with Moore's law: that the density of transistors in integrated circuits roughly doubles every 2 years. Big data has a similar "law" - the volume of data in existence roughly doubles every 2 years. In 2013 it was estimated that 4.4 zettabytes (1 trillion gigabytes) of data existed, and this was estimated reach 44 zettabytes by 2020 and 163 zettabytes by 2025. Growth in data storage requirements presents a serious challenge for traditional Enterprise IT where it now becomes critical to plan ahead in order to meet additional storage requirements, and this is where Cloud Computing shines.</p>

      <h2> Cloud Computing</h2>
      <p>Cloud computing, sometimes known as Infrastructure as a Service (IaaS) or Platform as a Service (PaaS), is one of the most disruptive technologies to have emerged over the last 12 years, pioneered by Amazon Amazon Web Services with Google and Microsoft a little way behind.</p>
      <p>The basic idea behind cloud computing is to provide scalable, fault-tolerant software and/or hardware, on demand via the internet, and the user pays only for the resources that are actually used. </p>
      <h3>Scalable:</h3>
      <p>In the traditional model, a company would own its own hardware, possibly located on site, with in-house system admin staff, or at a data centre and relying on data-centre sysadmin. If additional storage is needed, this will require a technician to physically add storage (which may require system down time). If additional processing capability is needed, then either the CPU/motherboard would require an upgrade, or an additional server may be required.  Cloud computing is scalable by design, and additional resources can be provisioned in a matter of minutes, rather than hours and days in the traditional model.</p>
      <h3>Fault-Tolerant:</h3>
      <p>Cloud computing resources such as Amazon AWS are highly fault tolerant. Amazon quotes 99.999999999% (the “11  9s”) durability which corresponds to an average annual expected loss of 0.000000001% of objects. For example, if you store 10,000 objects on Amazon S3, on average you can expect to incur a loss of one object once every 10,000,000 years.</p>
      <h3>On Demand:</h3>
      <p>Cloud computing resources are charged for based on actual use. Organisations can make substantial cost savings by transferring their computing to the cloud. Rather than buy an extremely large storage device in order to plan ahead for future growth, on the cloud the organisation is only charged for the storage actually used. When running a large data analytics process, the cloud will charge the organisation simply for the computing resources (eg CPU time) actually consumed.  An entire cluster of servers can be provisioned, data extracted/loaded, analytics code executed, files written, and finally the cluster is shut down, and the organisation is charged only for the resources it actually used during the job.</p>

      <h2> Serverless Computing</h2>
      <p>Traditional software development usually requires some knowledge of the system architecture that will run the software, even in the case of high-level programming languages. The idea of Serverless computing is to provide “Function as a Service” (FaaS) where applications are run in event-triggered stateless compute containers.  This allows the developer to focus on development rather than operating system and/or hardware issues. As an example, this website is completely serverless – it is written in HTML5, CSS3 and Javascript (jQuery)</p>

    <h2>Data Analysis, Data Engineering, Data Science, Statistical Modelling and Machine Learning</h2>

    <p>Data analysis/analytics is the process of systematically transforming raw data into usable information, by applying quantitative techniques that describe, illustrate, condense, recap and evaluate data and to discover useful information to  aid in organisational decision making.  Thus Data Analysis is a big picture process which is often broken down into sub-disciplines:</p>

    <p>Data engineering is the process of gathering and collecting data, storing it, processing it and making it available for analysis. Often the raw data will be located in separate locations in different formats, and the data engineering workflow will often involve accessing SQL and NoSQL databases.</p>
    <p>Data Science is the process of asking the right questions of any given dataset, and then answering them. In order to ask the right questions it is vitally important that the Data Scientist understand business processed well. And in order to answer these questions, the Data Scientist must be proficient in Statistical modelling and/or Machine Learning.</p>
    <p>Statistical modelling is the process of forming a hypothesis on a set of data, developing a statistical model (a mathematical description of the data-generating process) and to explain how the data could have come about and then testing it on the data to see if the hypothesis is consistent with the data. The aim of a model is to capture those aspects of a phenomenon that are relevant to inquiry.</p>
    <p>Machine Learning is the process of predicting things, usually based on actual measurements taken in the past and tries to find relationships and associations within the data. Statistical methods are often used, and the distinction between machine learning and statistical modelling is often vague, primarily because applied statistics is an allied- science, and the use and terminology within different disciplines often varies. In the Data Science world, statistics is usually used to describe a model where inference, rather than prediction, is the goal. However it should be noted that in other disciplines prediction may fall under statistics rather than machine learning.</p>
    <p>Machine Learning is subdivided into supervised learning (where we are given labelled training data) and unsupervised learning (where we are not given labelled data). For example, a supervised learning algorithm might be used to predict whether a person has a particular disease, or not, and we could use logistic regression for this, or another supervised learning algorithm such as Support Vector Machine, which has several benefits over logistic regression. Neural Networks also offer tremendous possibilities in supervised learning. Clustering and Dimensionality reduction are two of the most common unsupervised learning problems and there are a multitude of algorithms, such as k-Means and Hierarchical Clustering, and Principal Components Analysys.</p>
  
    <h2>Snowflake</h2>
    <p>Snowflake is a modern cloud-based data platform, offered as Software As A Service, that supports a massive range of solutions for data processing, data integration and analytics, and is capable of handling a diverse range of workloads including Data Engineering, Data Science, Applications and Data Sharing and Data Exchange.</p>
    <p>Snowflake decouples compute from storage in order to save costs and increase performance, and utilises a hybrid of shared-disk and shared-nothing architecture known as "mult-cluster shared data" whereby all compute nodes have access to all the data, while computations are carried out using massively-parallel processing.
    </p>

    <h2>Amazon AWS</h2>
    <p>Amazon Web Services is the clear leader in cloud computing. The AWS ecosystem is enormous with literally thousands of services available, from storage, computing and networking, to developer tools, management tools, mobile services, game development and analytics. Pinterest, Netflix, Spotify and AirBnB all use AWS to deliver their services. The website you are currently viewing is hosted on AWS as a serverless website using the AWS Simple Storage Service (S3)</p>


    <h2>R, C++, Python</h2>
    <p>R is a free, open source, environment for statistical computing and is available accross a wide variety of platforms including Linux/Unix, Windows and (Mac)OS X. R has benefitted from widespread adoption in the statistical computing community, and a vast number of libraries are available for implementing most statistical models and machine learning algorithms. A major attraction for R is the ability to integrate R with C++, in order to optimize code. R can be used with Hadoop and Spark, to bring enterprise level Big Data Anaytics into the mainstream.</p>
    <p>C++ is a general purpose programming language that supports models such as object-oriented programming. As a compiled language, C++ is known for it's run-time performance and is therefore very highly suited to data analytics where code may need to be optimised to run efficiently. </p>
    <p>Python is a general purpose programming language that has become extremely popular in the machine learning and data science arenas. Whereas R is the go to language for statistics, having evolved from the commercial S-Plus statistical software system, Python is the go to labguage for machine learning with very popular packages such as Pandas, Pytorch, Scikit-Learn, TensorFlow, although it shouod be noted that R is making very significatnt inroads into the machine learning world.</p>



  </body>

</html>